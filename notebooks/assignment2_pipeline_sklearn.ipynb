{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![car](http://www.auctionsamerica.com/images/lots/AF15/AF15_r0339_01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Data Science\n",
    "# Assignment 2: Building a machine learning pipeline with scikit-learn\n",
    "\n",
    "### In this assignment you will use some widely used sklearn classes to make implementing data science much easier.\n",
    "\n",
    "### Learning goals:\n",
    "- Understand the difference between transformer and estimator classes in sklearn\n",
    "- Learn how to build a pipeline\n",
    "- The importance of preproccesing: train-test split, scaling, one-hot-encoding\n",
    "- Understand how to implement regularization\n",
    "- Learn how to use cross validation and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\n",
    "\n",
    "Attribute Information:\n",
    "    1. mpg:           continuous\n",
    "    2. cylinders:     multi-valued discrete\n",
    "    3. displacement:  continuous\n",
    "    4. horsepower:    continuous\n",
    "    5. weight:        continuous\n",
    "    6. acceleration:  continuous\n",
    "    7. model year:    multi-valued discrete\n",
    "    8. origin:        multi-valued discrete\n",
    "    9. car name:      string (unique for each instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('../data/mtcars.csv', sep=';')\n",
    "\n",
    "cars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple check for existence of linear relations with a correlation plot\n",
    "cars_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get first intuition for the data\n",
    "cars_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "- there are no missing values\n",
    "- cylinders, model year and origin are discrete variables, so we may want to use **one hot encoding** for these features\n",
    "- especially weight is on a very different scale than the other features, so we want to use **scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set y\n",
    "y = cars_df['mpg'].values\n",
    "\n",
    "# drop the dependent variable and the unique car name to get the features\n",
    "X = cars_df.drop(['mpg', 'car name'], axis=1)\n",
    "\n",
    "# set which features we would like to use for one hot encoding and which for scaling\n",
    "features_ohe = ['cylinders', 'origin', 'model year']\n",
    "features_scaling = ['acceleration', 'weight', 'horsepower', 'displacement']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the scikit-learn estimator API\n",
    "\n",
    "In scikit-learn there are two kinds of classes: the **transformers** and the **estimators**.\n",
    "\n",
    "**Transformers**\n",
    "\n",
    "A transformer class has two essential methods: ```fit``` and ```transform```. The ```fit``` method is used to learn the parameters from the training data, and the ```transform``` method uses those paramters to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"../images/sklearnapi1.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimators**\n",
    "\n",
    "The estimators, such as the one we build in the first Hands-on, are very similar to the transformers. As you may recall, we also used the ```fit``` method to learn parameters of a model when we trained the estimator for regression. The other essential method of an estimator is ```predict```, which can provide us with the predictions of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"../images/sklearnapi2.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing:  Split train and test set\n",
    "The first step is to split the data in a train and a test set. Sklearn makes this easy for us using the ```train_test_split``` class. When using sklearn, it is very useful to check the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "Note that the train_test_split is not a transformer of an estimator, so we don't have to think about the ```fit```, ```transform``` or ```predict``` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# enter parameters of train_test_split (see example in documentation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(<CODE HERE>)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing: Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_boxplot(array, feature_names, title=None):\n",
    "    \"\"\"function to visualize the range of values of features\"\"\"\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(array, vert=False);\n",
    "    plt.grid(True)\n",
    "    plt.xticks(size=16);\n",
    "    plt.yticks(size=16);\n",
    "    plt.xlabel('feature values', fontsize=18)\n",
    "    ax.set_yticklabels(feature_names)\n",
    "    plt.title(title, fontsize=22)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize range of values of features\n",
    "plot_boxplot(X_train[features_scaling].values, feature_names=features_scaling, title='Value ranges of raw features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to see that the features are on quite different scales. Let's use sklearns transformers [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to see how this will change the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Start with standardization. Create an instance of the standard scaler class\n",
    "stsc = StandardScaler()\n",
    "# use fit to let the instane calculate the parameters to transform the features \n",
    "stsc.fit(X_train[features_scaling])\n",
    "# use transform to return the transformation\n",
    "X_train_stsc = stsc.transform(<CODE HERE>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the standardized \n",
    "plot_boxplot(X_train_stsc, features_scaling, title='Value ranges of standardized features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now use MinMaxScaling to check the difference. First create an instance.\n",
    "mmsc = <CODE HERE>\n",
    "# use the fit_transform method to return the transformation\n",
    "# fit_transform is a method of sklearn transformer classes to directly fit the parameters and return the transformation\n",
    "X_train_mmsc = <CODE HERE>\n",
    "\n",
    "plot_boxplot(X_train_mmsc, features_scaling, title='Value ranges of min-max scaled features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "Now we have taken care of the continuous features, it is time to focus on the categorical features. [Here](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) is the documentation for the OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create instancce of OneHotEncoder class\n",
    "ohe = <CODE HERE>\n",
    "# Return the one hote ncoded features\n",
    "X_train_ohe = <CODE HERE>\n",
    "\n",
    "# print shape to check how many new features will be used\n",
    "print(X_train_ohe.shape)\n",
    "\n",
    "X_train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get total preprocessed X_train by concatenating the scaled features with the one hot encoded features.\n",
    "# You can choose to use the the standardized version or the minmax version\n",
    "X_train_pp = <CODE HERE>\n",
    "\n",
    "# print features of the first observation\n",
    "X_train_pp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "The data is preprocessed! We are going to train a linear regression model. Here is the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create an instance of LinearRegression\n",
    "lr = <CODE HERE>\n",
    "# fit a linear model\n",
    "lr.<CODE HERE>\n",
    "\n",
    "# get predictions on training set\n",
    "y_train_pred = lr.<CODE HERE>\n",
    "\n",
    "# calclate mean squared error of training set\n",
    "mse_train = mean_squared_error(<CODE HERE>)\n",
    "print('MSE train set: {:.3f}'.format(mse_train))\n",
    "\n",
    "# print the weights (coef_ in documentation) of the linear model\n",
    "<CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set\n",
    "With sklearn it is now is very easy to repeat the same steps for the test set. We already fitted our scaler, one hot encoder and our model with our training set. Now we just have to use these fits to get a prediction for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get scaled features for test set\n",
    "X_test_stsc = stsc.<CODE HERE>\n",
    "# get ohe features for test set\n",
    "X_test_ohe = ohe.<CODE HERE>\n",
    "# concatenate features\n",
    "X_test_pp = <CODE HERE>\n",
    "\n",
    "# get predictions on test set with train model\n",
    "y_test_pred = lr.<CODE HERE>\n",
    "\n",
    "# calculate mean squared error\n",
    "mse_test = mean_squared_error(<CODE HERE>)\n",
    "print('MSE test set: {:.3f}'.format(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting hyperparameters\n",
    "\n",
    "So it seems there is some overfitting involved, but very little. One way to reduce this problem is with regularization. When using linear regression, there are two different estimators in sklearn for L1 and L2 regression: [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), respectively. We will use the L2 regression (Ridge) here.\n",
    "\n",
    "### Cross validation\n",
    "When tuning hyperparameter, such as the regularization parameter, we want to evaluate using cross validation. Creating the folds in the traing set can easily be done using [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create instance of kfold\n",
    "k_fold = KFold(n_splits=3, shuffle=True)\n",
    "# create instance of linear regression with L2 regularization (Ridge)\n",
    "# choose appropriate alpha parameter(regularization parameter) like 1 or 10\n",
    "lr_l2 = <CODE HERE>\n",
    "# create list to track cross validation mean squared errors\n",
    "cv_mse = []\n",
    "\n",
    "# kfold.split() returns indices of train and cross validation set\n",
    "for train_indices, cv_indices in k_fold.split(X_train_pp):\n",
    "    # fit model on train\n",
    "    lr_l2.fit(X_train_pp[train_indices], y_train[train_indices])\n",
    "    # get predictions for fold\n",
    "    y_pred_cv = lr_l2.predict(X_train_pp[cv_indices])\n",
    "    # keep track of mean squared error on cross validation set\n",
    "    cv_mse.append(mean_squared_error(y_pred_cv, y_train[cv_indices]))\n",
    "    \n",
    "print('MSE of each cross validation fold:\\n{}'.format(cv_mse))\n",
    "\n",
    "# calucalte mean of cross validation mean squared errors\n",
    "mse_cv_l2 = sum(cv_mse) / len(cv_mse)\n",
    "print('MSE CV: {:.3f}'.format(mse_cv_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tune the hyperparameter without 'seeing' the test set, we can use the tuned hyperparameter to check how well it does on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit linear regression with best regularization parameter on entire training set\n",
    "lr_l2.fit(X_train_pp, y_train)\n",
    "# get predictions\n",
    "y_train_pred_l2 = lr_l2.predict(X_train_pp)\n",
    "y_test_pred_l2 = lr_l2.predict(X_test_pp)\n",
    "# get mean squared errors of data sets\n",
    "mse_train_l2 = mean_squared_error(y_train, y_train_pred_l2)\n",
    "mse_test_l2 = mean_squared_error(y_test, y_test_pred_l2)\n",
    "print('MSE train: {:}\\nMSE CV: {:}\\nMSE test: {:}'.format(mse_train_l2, mse_cv_l2, mse_test_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Play around with the regularization parameter a little**\n",
    "\n",
    "** What happens with the train, cross validation and test mean squared errors? **\n",
    "\n",
    "** What can you conclude from this? **"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
