{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Data Science\n",
    "# Assignment 4: Dimensionality reduction and unsupervised learning\n",
    "\n",
    "### In this assignment you will experiment with the most often used techniques for dimensionality reduction and unsupervised learning: PCA and k-means.\n",
    "\n",
    "### Learning goals:\n",
    "\n",
    "* PCA\n",
    "    * Understand how to use sklearn to apply PCA to a dataset\n",
    "    * Experiment with different datasets to understand the output of PCA and how it can be useful\n",
    "    * Learn how to choose the number of principal componets\n",
    "\n",
    "* k-means\n",
    "    * Understand how to use sklearn to apply k-means to a dataset  \n",
    "    * Learn how to choose an appropriate number of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA\n",
    "Before you begin, take a look at the sklearn website and check which input parameters you can use when performing a PCA and which output you can expect:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Starting easy:\n",
    "Here the goal is to perform a simple PCA and get acquainted with the input and putput of such a model. The steps to be followed are:\n",
    "\n",
    "1. Load dataset\n",
    "2. Define PCA object (start by using n_components = 3)\n",
    "3. Fit PCA to data\n",
    "3. Print the variance explained by each component\n",
    "4. Project the original feature matrix onto the PCA coordinates\n",
    "5. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "print('Dataset shape: {}'.format(X_iris.shape))\n",
    "\n",
    "# 2. Choose number of components:\n",
    "pca_components_num = <NUMBER OF FEATURES HERE>\n",
    "\n",
    "# 3. Define pca:\n",
    "pca = PCA(<INPUT HERE>)\n",
    "\n",
    "# 4. Fit to data:\n",
    "pca.fit(<FEATURE MATRIX>)\n",
    "\n",
    "# 5. Print the variance explained:\n",
    "print('Percentage of variance explained per component:{}'.format(<CODE HERE>))\n",
    "\n",
    "# 6. Project the original feature matrix onto the PCA coordinates:\n",
    "X_iris_pca = pca.transform(<FEATURE MATRIX>)\n",
    "    ### check: do you understand what .transform() does?\n",
    "\n",
    "# 7. Visualize the result:\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X_iris_pca[y_iris == label, 0].mean(),\n",
    "              X_iris_pca[y_iris == label, 1].mean() + 1.5,\n",
    "              X_iris_pca[y_iris == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y_iris = np.choose(y_iris, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], X_iris_pca[:, 2], c=y_iris, cmap=plt.cm.spectral)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which conclusions can you draw from this exercise?\n",
    "Even though the iris dataset is quite simple (4 features!), you can already see from the result of PCA that one component can explain a big part of the variance (92%). And the plot also shows that the variance along this component is strongly correlated with the label.\n",
    "Not so impressive for iris, maybe. But think of a scenario in which your dataset does not have 4 features, but 400 and you can combine a great deal of the variance in a couple of components. That can decrease the complexity of you data greatly and make it easier for a model to find a good fit, since the feature space is much smaller using the principal components than the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extra: choosing the number of principal components\n",
    "An important part of performing a PCA is choosing how many components should be carried forward into the next steps of your pipeline. To experiemnt with that, we will now work with the dataset \"digits\", which has 64 features. The idea is to perform a PCA with many components and use the amount of variance explained by each of them in order to choose how many should be kept.\n",
    "\n",
    "You should perform the following steps:\n",
    "1. Load the dataset\n",
    "2. Define a PCA object, but without specifying the number of components (check sklearn's wesite to see how many components will be used).\n",
    "3. Make 2 plots:\n",
    "    * plot 1: y = variance explained by each component vs x = each component (1,2,3...)\n",
    "    * plot 2: y = cumulative variance explained (sum for all components) vs x = total number 4. Use the curves to reflect about how many components you would keep for such a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Load dataset\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "print('Dataset shape: {}'.format(X_digits.shape))\n",
    "\n",
    "\n",
    "# 2. Define PCA, without specifying the number of components. \n",
    "# When you do that, n_components == min(n_samples, n_features)\n",
    "\n",
    "# 3. Fit to data:\n",
    "\n",
    "# 4. Plot:\n",
    "# x = compontent number [1,...,n_components] vs y = variance explained by each component\n",
    "\n",
    "\n",
    "# 5. Plot the cumulative explained variance:\n",
    "# x = total of number of components (e.g. 3) vs \n",
    "# y = variance explained by all those components (var_x1 + var_x2 + var_x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-means\n",
    "K-means is an unsupervised technique, which means that it does not use any ground truth information (true labels).\n",
    "For learning purposes, we will keep on working with the iris dataset, for which we do know the ground truth. We will first ignore the class label, perform k-means and later check if the clusters found have any relationship with the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Starting easy!\n",
    "In this session we will:\n",
    "\n",
    "1. Perform k-means with n_clusters = 3 (because we know there are 3 classes!)\n",
    "2. Get the labels attributed to each sample\n",
    "3. Make a 3D plot where: x,y,z are 3 of the 4 features of the dataset and the color of each point (=sample) corresponds to the label given by k-means\n",
    "4. Make the same plot from (3), but now color the points accoriding to the true label. Do you see a relationship?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a K-means object with 3 clusters:\n",
    "estimator = KMeans(n_clusters=<NUMBER OF CLUSTERS HERE>)\n",
    "\n",
    "# Fit to data:\n",
    "estimator.fit(X_iris)\n",
    "\n",
    "# Print the inertia:\n",
    "print('Inertia: {}'.format(<CODE HERE>))\n",
    "\n",
    "# Get labels attributed to each sample:\n",
    "labels = <CODE HERE>\n",
    "\n",
    "# Plot a 3D graph where x,y,z are 3 of the 4 features of iris and the color of \n",
    "# each point/sample represents the label given to it by k-means\n",
    "\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "plt.cla()\n",
    "\n",
    "ax.scatter(X_iris[:, 3], X_iris[:, 0], X_iris[:, 2], c=labels.astype(np.float))\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "\n",
    "# Plot the same graph above, but where the color of each point/sample represents \n",
    "# the true class label\n",
    "\n",
    "fig = plt.figure(2, figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X_iris[y_iris == label, 3].mean(),\n",
    "              X_iris[y_iris == label, 0].mean() + 1.5,\n",
    "              X_iris[y_iris == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "    \n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y_iris_reorder = np.choose(y_iris, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X_iris[:, 3], X_iris[:, 0], X_iris[:, 2], c=y_iris_reorder)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bonus 1: \n",
    "In the previous session we could specify the number of clusters because we knw how many clusters to expect. In a real problem, how would you do that?\n",
    "\n",
    "In this session we will generate several k-means models with different number of clusters and compute the inertia or within-cluster sum-of-squares (check session 2.2 for more details: http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "We will perform the following steps:\n",
    "1. Make several values for the number of clusters (e.g. from 1 to 8)\n",
    "2. For each model, compute the inertia\n",
    "3. Make an elbow plot (inertia vs number of clusters). how many clsuters would you choose if you didn't have any previous knowledge?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
