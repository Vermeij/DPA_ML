{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Data Science\n",
    "# Assignment 3: Classification\n",
    "\n",
    "In this assignment you will work with the \"titanic\" dataset and will use information over each passenger in prder to predict if he/she died or survived the shipwrek.\n",
    "\n",
    "### Learning goals:\n",
    "* Experiment with feature engineering (encoding, scaling, missing values)\n",
    "* Use sklearn to implement a classification model\n",
    "* Validate your model using cross-validation\n",
    "* Generate predictions for a new dataset\n",
    "* Prepare output of peredictions to submit it to Kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Introduction: Kaggle Competition | Titanic Machine Learning from Disaster\n",
    "\n",
    ">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    ">One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    ">In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    ">This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n",
    "\n",
    "From the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n",
    "\n",
    "\n",
    "### Goal for this Notebook:\n",
    "Show a simple example of an analysis of the Titanic disaster in Python using a full complement of PyData utilities. This is aimed for those looking to get into the field or those who are already in the field and looking to see an example of an analysis done with Python.\n",
    "\n",
    "#### This Notebook will show basic examples of: \n",
    "#### Data Handling\n",
    "*   Importing Data with Pandas\n",
    "*   Cleaning Data\n",
    "*   Exploring Data through Visualizations with Matplotlib\n",
    "\n",
    "#### Data Analysis\n",
    "*    Supervised Machine learning Techniques:\n",
    "    +   Building classification models\n",
    "\n",
    "#### Valuation of the Analysis\n",
    "*   K-folds cross validation to valuate results locally\n",
    "*   Output the results from the IPython Notebook to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from statsmodels.nonparametric import smoothers_lowess\n",
    "from pandas import Series, DataFrame\n",
    "from patsy import dmatrices\n",
    "from sklearn import datasets, svm\n",
    "from KaggleAux import predict as ka # see github.com/agconti/kaggleaux for more details\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler, Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\") \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Missing values\n",
    "There are several techniques to deal with missing values, but they will not be covered in this course. In this assinment we will use the simplest strategy possible: eliminate features with a big proportion of missing values and later eliminate all samples that still have missing values.\n",
    "\n",
    "Perform the following steps:\n",
    "1: Check the amount of missing values per feature\n",
    "2: Drop columns with too many missing values\n",
    "3: Drop all rows that have missing values in the remaining columns\n",
    "\n",
    "* EXTRA: you can also try to fill in the missing values, check: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Check the amount of missing values per feature:\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Drop columns with too many missing values:\n",
    "df = df.drop(<CODE HERE>)\n",
    "\n",
    "# 3. Drop all rows that have missing values in the remaining columns:\n",
    "df = <CODE HERE>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Visualisation\n",
    "It is very useful to get an idea about the relationship between each feature and the label you are trying to predict. A powerful tool to do that is by means of plots, such as histograms, barplots and boxplots, where you try to see if there is a difference in the distribution of the values of a certain feature accoring to the label. But be careful: the best plot depends on the type of the variable. Is the variable chategorical? Is it continuous? Numerical, but with just a couple of levels?\n",
    "\n",
    "Perform the following steps:\n",
    "1. Choose which variables you would like to plot.\n",
    "2. Choose a type of plot for each variable.\n",
    "3. Make the plot!\n",
    "EXTRA: maybe the power in is the combination fo features? Think if a new feature that combines other features, plot it and check if your idea was correct. Tip: combination of 'Sex' and 'Pclass'\n",
    "\n",
    "Reflection: looking at the plots, can you already identify which features areprobably useful to predict the label?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Countplot of 'Pclass' and 'Sex':\n",
    "fig = plt.figure()\n",
    "\n",
    "idx = 0\n",
    "for feature in ['Pclass','Sex']:\n",
    "    ax = fig.add_subplot(2, 2, idx+1)\n",
    "    sns.countplot(x=feature, data=df,hue='Survived')\n",
    "    idx +=1\n",
    "\n",
    "# Boxplot of violinplot of 'Age' and 'Fare'\n",
    "<CODE HERE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Feature engineering\n",
    "\n",
    "Categorical features have to be encoded before they are used in the model and the best way of encoding them depends of the feature itself.\n",
    "\n",
    "We will start by encoding two features: 'Sex' and 'Embarked'.\n",
    "\n",
    "Perform the following steps:\n",
    "1. Start encoding 'Sex'. \n",
    "    * How many levels does it have? How should you encode it?\n",
    "2. Encode 'Embarked'.\n",
    "     * How many levels does it have? How should you encode it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 'Sex' has only two levels: so a good option is: female=0, male=1\n",
    "<CODE HERE>\n",
    "\n",
    "# 'Embarked' has 3 levels. If we encode them as 0,1,2, we imply that \n",
    "# there is an order between the levels and the model will try to learn from that. \n",
    "# In this case, we should use one hot econding then.\n",
    "# Tip: use pandas' get_dummies()\n",
    "<CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "Now it is time to experiment with different classification models!\n",
    "\n",
    "Attention points:\n",
    "* Start by choosing one classifier (logistic regression, random forest?)\n",
    "* Don't forget to start with a dummy classifier in order to have a baseline\n",
    "* Use proper cross-validation  to make sure that you are not overfitting your training data. \n",
    "* For each model, use grid search in order to determine the best hyperparameters\n",
    "* Think of scaling your data. Is it important in this case? If so, which technique should you use? And how do you implement it in a pipeline to make sure that there is no leakage between training and test sets in the cross-validation step?\n",
    "* Thinking of using the structure of the sklearn pipeline (example here: http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html)\n",
    "\n",
    "Perform the following steps:\n",
    "1. Choose the feartures you will use in the model\n",
    "2. From the dataframe, make X and y\n",
    "3. Define your scaler (if any)\n",
    "4. Choose the classifier to be used\n",
    "5. Define the grid of hyperparameters\n",
    "6. Perform grid search with cross-validation\n",
    "7. Check which where the best hyperparameters and trian the final model in the whole training set\n",
    "EXTRA: make predictions of the hold-out set to submit it to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label = <LABEL NAME HERE>\n",
    "features = <LIST OF FEATURES HERE>\n",
    "X = df.loc[:,features].values\n",
    "y = np.array(df[label])\n",
    "n_features = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing steps:\n",
    "scaler = <CODE HERE>\n",
    "preproc_steps = [('my_scaler', scaler)]\n",
    "\n",
    "\n",
    "\n",
    "# specification of different model types and their defaults\n",
    "# make a dictionary of all models you want to have available in your pipeline\n",
    "model_steps_dict = {'dummy': [('dummy', DummyClassifier(strategy='prior'))],\n",
    "                    'lr': [('lr', LogisticRegression(C=0.001, penalty='l2', tol=0.01,\n",
    "                                                     class_weight=None))],\n",
    "                    'new_model':[('new_model',<DEFINE NEW MODEL HERE>)]\n",
    "                   }\n",
    "\n",
    "# specification of the different model hyperparameters and tuning space\n",
    "model_params_grid = {'dummy': {},\n",
    "                    'lr': {'lr__C': [1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "                    'new_model': {'new_model__PARAM1': <GRID FOR PARAM1 HERE>,\n",
    "                           'new_model__PARAM2': <GRID FOR PARAM2 HERE>}\n",
    "                     }\n",
    "\n",
    "# store the model step\n",
    "model_steps = model_steps_dict[model_type]\n",
    "\n",
    "# combine everything in one pipeline\n",
    "estimator = Pipeline(steps=(preproc_steps + model_steps))\n",
    "\n",
    "# Choose here the model you will use:\n",
    "model_type = 'dummy'\n",
    "\n",
    "print estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross-validation:\n",
    "\n",
    "k_cv = <NUMBER OF FOLDS HERE>\n",
    "\n",
    "grid_search = GridSearchCV(estimator, cv=k_cv,\n",
    "                         param_grid=model_params_grid[model_type])\n",
    "\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Best score: {}'.format(<CODE HERE>))\n",
    "print('\\nBest parameters: {}'.format(<CODE HERE>))\n",
    "print('\\nBest estimator: {}'.format(<CODE HERE>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final model:\n",
    "model = grid_search.best_estimator_.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EXTRA: Making predictions on the true test set + submission to Kaggle:\n",
    "In the 'data' folder there is a file with unlabeled data: test.csv\n",
    "You can use this file to make predictions and submit your results to kaggle.\n",
    "Be careful: when working with a new dataset, make sure that you will preprocess it in the same way you did with the training data. And if there are missing values in the features used by your model, you need now to impute them in with information you have from your training set, otherwise you won't be able to make a prediction for all samples.\n",
    "\n",
    "Perfom the following steps:\n",
    "1. Load the data\n",
    "2. Drop features 'Ticket','Cabin'\n",
    "3. Check if there is still missing data. If so, how can you impute it? (tip 1: you can fill in the missing values with the average value of that feature in the training set. tip 2 (advanced): if you now that two features are very correlated (e.g. 'Plcass' and 'Fare'), you can use this information to make an informed-imputation (Fare|Pclass_1 = mean Fare on training set when Pclass=1)\n",
    "4. Encode 'Gender' and 'Embarked'\n",
    "5. Feature scaling (tip: use fit().transform())\n",
    "6. Use model to make predictions\n",
    "7. Generate output fro kaggle: dataframe with two columns: 'PassengerID' and 'Survided' (=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test.csv')\n",
    "print(df_test.shape)\n",
    "print(df_test.isnull().sum())\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop features 'Ticket','Cabin'\n",
    "df_test = df_test.drop(<CODE HERE>)\n",
    "\n",
    "# Age: missing values = mean of 'Age' in the training set\n",
    "df_test['Age'] = <CODE HERE>\n",
    "\n",
    "# Fare: missing values = mean w.r.t. 'Pclass':\n",
    "<CODE HERE>\n",
    "\n",
    "# Feature encoding for 'Gender' and 'Embarked'\n",
    "<CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selection of features + scaling\n",
    "X_test = df_test.loc[:,features].values\n",
    "\n",
    "X_test = scaler.fit(X).transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction:\n",
    "output = <CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output for kaggle:\n",
    "result = np.c_[df_test.PassengerId.astype(int), output.astype(int)]\n",
    "\n",
    "df_result = pd.DataFrame(result[:,0:2], columns=['PassengerId', 'Survived'])\n",
    "# df_result.to_csv('../results/titanic_1-4.csv', index=False)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
